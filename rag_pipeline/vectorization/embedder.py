"""
Embedder Module: Vector h√≥a d·ªØ li·ªáu text th√†nh embeddings

Module n√†y ch·ªãu tr√°ch nhi·ªám:
- Load embedding models
- Vector h√≥a text chunks
- X·ª≠ l√Ω b·∫£ng v√† content ƒë·∫∑c bi·ªát
- L∆∞u tr·ªØ vectorized data
- Batch processing cho hi·ªáu su·∫•t t·ªëi ∆∞u
"""

import json
import pickle
import os
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
from sentence_transformers import SentenceTransformer

from utils.logger import default_logger as logger


def load_embedding_model(model_name: str = "sentence-transformers/all-MiniLM-L6-v2") -> SentenceTransformer:
    """
    Load embedding model t·ª´ sentence-transformers
    
    Args:
        model_name: T√™n model ƒë·ªÉ load
            - "sentence-transformers/all-MiniLM-L6-v2": 384 dimensions, nh·∫π v√† nhanh
            - "sentence-transformers/all-mpnet-base-v2": 768 dimensions, ch·∫•t l∆∞·ª£ng cao h∆°n
            - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": Multilingual support
            
    Returns:
        SentenceTransformer model ƒë√£ load
        
    Raises:
        Exception: N·∫øu kh√¥ng th·ªÉ load model
    """
    logger.info(f"Loading embedding model: {model_name}")
    
    try:
        model = SentenceTransformer(model_name)
        logger.info(f"Successfully loaded model: {model_name}")
        logger.info(f"Model max sequence length: {model.max_seq_length}")
        return model
        
    except Exception as e:
        error_msg = f"L·ªói khi load embedding model {model_name}: {str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)


def preprocess_content_for_embedding(content: str) -> str:
    """
    Ti·ªÅn x·ª≠ l√Ω content tr∆∞·ªõc khi vector h√≥a
    
    Args:
        content: Text content c·∫ßn x·ª≠ l√Ω
        
    Returns:
        Content ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    # X·ª≠ l√Ω b·∫£ng Markdown
    processed_content = process_markdown_tables(content)
    
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y nhi·ªÖu
    processed_content = clean_content_for_embedding(processed_content)
    
    return processed_content


def process_markdown_tables(content: str) -> str:
    """
    X·ª≠ l√Ω b·∫£ng Markdown v√† chuy·ªÉn th√†nh text c√≥ c·∫•u tr√∫c
    
    Args:
        content: Content ch·ª©a b·∫£ng Markdown
        
    Returns:
        Content v·ªõi b·∫£ng ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    import re
    
    lines = content.split('\n')
    processed_lines = []
    in_table = False
    table_headers = []
    
    for line in lines:
        # Ph√°t hi·ªán b·∫£ng Markdown (line c√≥ | v√† kh√¥ng ph·∫£i separator)
        if '|' in line and not re.match(r'^\s*\|[\s\-\|]+\|\s*$', line):
            if not in_table:
                # B·∫Øt ƒë·∫ßu b·∫£ng m·ªõi
                in_table = True
                table_headers = [header.strip() for header in line.split('|')[1:-1]]
                processed_lines.append(f"B·∫£ng d·ªØ li·ªáu v·ªõi c√°c c·ªôt: {', '.join(table_headers)}")
            else:
                # D√≤ng d·ªØ li·ªáu trong b·∫£ng
                cells = [cell.strip() for cell in line.split('|')[1:-1]]
                if len(cells) == len(table_headers):
                    row_text = "; ".join([f"{header}: {cell}" for header, cell in zip(table_headers, cells) if cell])
                    if row_text:
                        processed_lines.append(row_text)
        elif in_table and '|' not in line:
            # K·∫øt th√∫c b·∫£ng
            in_table = False
            table_headers = []
            processed_lines.append(line)
        else:
            processed_lines.append(line)
    
    return '\n'.join(processed_lines)


def clean_content_for_embedding(content: str) -> str:
    """
    L√†m s·∫°ch content ƒë·ªÉ t·ªëi ∆∞u cho embedding
    
    Args:
        content: Content c·∫ßn l√†m s·∫°ch
        
    Returns:
        Content ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
    """
    import re
    
    # Lo·∫°i b·ªè multiple spaces
    content = re.sub(r' +', ' ', content)
    
    # Lo·∫°i b·ªè multiple newlines (gi·ªØ l·∫°i c·∫•u tr√∫c ƒëo·∫°n vƒÉn)
    content = re.sub(r'\n{3,}', '\n\n', content)
    
    # Lo·∫°i b·ªè trailing/leading whitespaces
    content = content.strip()
    
    return content


def extract_texts_from_chunks(chunks: List[Dict[str, Any]]) -> List[str]:
    """
    Tr√≠ch xu·∫•t texts t·ª´ danh s√°ch chunks ƒë·ªÉ vector h√≥a
    
    Args:
        chunks: List c√°c chunks v·ªõi structure {"content": str, "metadata": dict}
        
    Returns:
        List c√°c text strings ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
    """
    logger.info(f"Extracting texts from {len(chunks)} chunks for embedding")
    
    texts = []
    for i, chunk in enumerate(chunks):
        if "content" not in chunk:
            logger.warning(f"Chunk {i} kh√¥ng c√≥ 'content' field")
            continue
            
        # Preprocess content
        processed_content = preprocess_content_for_embedding(chunk["content"])
        texts.append(processed_content)
    
    logger.info(f"Extracted {len(texts)} texts for embedding")
    return texts


def generate_embeddings_batch(
    texts: List[str], 
    model: SentenceTransformer,
    batch_size: int = 32,
    show_progress: bool = True
) -> np.ndarray:
    """
    T·∫°o embeddings cho list texts v·ªõi batch processing
    
    Args:
        texts: List c√°c text c·∫ßn vector h√≥a
        model: SentenceTransformer model
        batch_size: K√≠ch th∆∞·ªõc batch ƒë·ªÉ x·ª≠ l√Ω
        show_progress: Hi·ªÉn th·ªã progress bar
        
    Returns:
        numpy array ch·ª©a embeddings
    """
    logger.info(f"Generating embeddings for {len(texts)} texts (batch_size={batch_size})")
    
    try:
        # Encode t·∫•t c·∫£ texts
        embeddings = model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )
        
        logger.info(f"Successfully generated embeddings: shape {embeddings.shape}")
        return embeddings
        
    except Exception as e:
        error_msg = f"L·ªói khi generate embeddings: {str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg)


def attach_embeddings_to_chunks(
    chunks: List[Dict[str, Any]], 
    embeddings: np.ndarray
) -> List[Dict[str, Any]]:
    """
    G·∫Øn embeddings v√†o chunks
    
    Args:
        chunks: List chunks g·ªëc
        embeddings: numpy array embeddings t∆∞∆°ng ·ª©ng
        
    Returns:
        List chunks ƒë√£ ƒë∆∞·ª£c g·∫Øn embeddings
    """
    if len(chunks) != len(embeddings):
        raise ValueError(f"S·ªë l∆∞·ª£ng chunks ({len(chunks)}) kh√¥ng kh·ªõp v·ªõi s·ªë embeddings ({len(embeddings)})")
    
    logger.info(f"Attaching embeddings to {len(chunks)} chunks")
    
    vectorized_chunks = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        # Copy chunk g·ªëc
        vectorized_chunk = chunk.copy()
        
        # Th√™m embedding (convert to list ƒë·ªÉ serialize JSON)
        vectorized_chunk["embedding"] = embedding.tolist()
        
        # Th√™m metadata v·ªÅ embedding
        if "metadata" not in vectorized_chunk:
            vectorized_chunk["metadata"] = {}
            
        vectorized_chunk["metadata"].update({
            "embedding_model": "sentence-transformers",
            "vector_dimensions": len(embedding),
            "chunk_index": i
        })
        
        vectorized_chunks.append(vectorized_chunk)
    
    logger.info(f"Successfully attached embeddings to all chunks")
    return vectorized_chunks


def save_vectorized_data_pickle(vectorized_chunks: List[Dict[str, Any]], output_path: str) -> bool:
    """
    L∆∞u vectorized data d∆∞·ªõi d·∫°ng pickle (bao g·ªìm c·∫£ embeddings)
    
    Args:
        vectorized_chunks: List chunks ƒë√£ vector h√≥a
        output_path: ƒê∆∞·ªùng d·∫´n file pickle output
        
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    logger.info(f"Saving vectorized data to pickle: {output_path}")
    
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, "wb") as f:
            pickle.dump(vectorized_chunks, f)
        
        logger.info(f"Successfully saved vectorized data to {output_path}")
        return True
        
    except Exception as e:
        error_msg = f"L·ªói khi l∆∞u pickle file: {str(e)}"
        logger.error(error_msg)
        return False


def save_vectorized_metadata_json(vectorized_chunks: List[Dict[str, Any]], output_path: str) -> bool:
    """
    L∆∞u metadata c·ªßa vectorized data d∆∞·ªõi d·∫°ng JSON (kh√¥ng bao g·ªìm embeddings)
    
    Args:
        vectorized_chunks: List chunks ƒë√£ vector h√≥a
        output_path: ƒê∆∞·ªùng d·∫´n file JSON output
        
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    logger.info(f"Saving vectorized metadata to JSON: {output_path}")
    
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Remove embeddings ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc file
        metadata_chunks = []
        for chunk in vectorized_chunks:
            chunk_copy = chunk.copy()
            if "embedding" in chunk_copy:
                del chunk_copy["embedding"]
            metadata_chunks.append(chunk_copy)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(metadata_chunks, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Successfully saved metadata to {output_path}")
        return True
        
    except Exception as e:
        error_msg = f"L·ªói khi l∆∞u JSON file: {str(e)}"
        logger.error(error_msg)
        return False


def get_embedding_statistics(embeddings: np.ndarray) -> Dict[str, Any]:
    """
    T√≠nh to√°n th·ªëng k√™ v·ªÅ embeddings
    
    Args:
        embeddings: numpy array embeddings
        
    Returns:
        Dictionary ch·ª©a th·ªëng k√™
    """
    if embeddings.size == 0:
        return {"error": "Empty embeddings array"}
    
    stats = {
        "total_vectors": len(embeddings),
        "vector_dimensions": embeddings.shape[1] if len(embeddings.shape) > 1 else 0,
        "mean_norm": float(np.mean(np.linalg.norm(embeddings, axis=1))),
        "std_norm": float(np.std(np.linalg.norm(embeddings, axis=1))),
        "min_value": float(np.min(embeddings)),
        "max_value": float(np.max(embeddings)),
        "mean_value": float(np.mean(embeddings))
    }
    
    return stats


def display_vectorization_summary(
    vectorized_chunks: List[Dict[str, Any]], 
    embeddings: np.ndarray,
    model_name: str,
    pickle_path: str,
    json_path: str
) -> None:
    """
    Hi·ªÉn th·ªã t√≥m t·∫Øt qu√° tr√¨nh vector h√≥a
    
    Args:
        vectorized_chunks: List chunks ƒë√£ vector h√≥a
        embeddings: numpy array embeddings
        model_name: T√™n model ƒë√£ s·ª≠ d·ª•ng
        pickle_path: ƒê∆∞·ªùng d·∫´n file pickle
        json_path: ƒê∆∞·ªùng d·∫´n file JSON
    """
    stats = get_embedding_statistics(embeddings)
    
    logger.info("=" * 70)
    logger.info("‚úÖ VECTOR H√ìA HO√ÄN T·∫§T")
    logger.info("=" * 70)
    logger.info(f"   Model: {model_name}")
    logger.info(f"   Total chunks: {len(vectorized_chunks)}")
    logger.info(f"   Vector dimensions: {stats.get('vector_dimensions', 'N/A')}")
    logger.info(f"   Mean vector norm: {stats.get('mean_norm', 'N/A'):.4f}")
    logger.info("=" * 70)
    logger.info("üìÅ Files saved:")
    logger.info(f"   Pickle (with embeddings): {pickle_path}")
    logger.info(f"   JSON (metadata only): {json_path}")
    logger.info("=" * 70)


def vectorize_chunks_pipeline(
    chunks: List[Dict[str, Any]],
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    output_dir: str = "data/vector_store",
    batch_size: int = 32,
    save_pickle: bool = True,
    save_json: bool = True
) -> Dict[str, Any]:
    """
    Pipeline ho√†n ch·ªânh ƒë·ªÉ vector h√≥a chunks
    
    Args:
        chunks: List chunks c·∫ßn vector h√≥a
        model_name: T√™n embedding model
        output_dir: Th∆∞ m·ª•c l∆∞u output files
        batch_size: K√≠ch th∆∞·ªõc batch ƒë·ªÉ x·ª≠ l√Ω
        save_pickle: C√≥ l∆∞u file pickle kh√¥ng
        save_json: C√≥ l∆∞u file JSON kh√¥ng
        
    Returns:
        Dictionary ch·ª©a k·∫øt qu·∫£ v√† th·ªëng k√™
    """
    logger.info("üöÄ B·∫Øt ƒë·∫ßu vectorization pipeline")
    
    if not chunks:
        logger.error("Kh√¥ng c√≥ chunks ƒë·ªÉ vector h√≥a")
        return {"success": False, "error": "Empty chunks list"}
    
    try:
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # B∆∞·ªõc 1: Load embedding model
        model = load_embedding_model(model_name)
        
        # B∆∞·ªõc 2: Extract v√† preprocess texts
        texts = extract_texts_from_chunks(chunks)
        
        # B∆∞·ªõc 3: Generate embeddings
        embeddings = generate_embeddings_batch(texts, model, batch_size)
        
        # B∆∞·ªõc 4: Attach embeddings to chunks
        vectorized_chunks = attach_embeddings_to_chunks(chunks, embeddings)
        
        # B∆∞·ªõc 5: Save data
        pickle_path = os.path.join(output_dir, "vectorized_data.pkl")
        json_path = os.path.join(output_dir, "vectorized_metadata.json")
        
        pickle_success = True
        json_success = True
        
        if save_pickle:
            pickle_success = save_vectorized_data_pickle(vectorized_chunks, pickle_path)
        
        if save_json:
            json_success = save_vectorized_metadata_json(vectorized_chunks, json_path)
        
        # B∆∞·ªõc 6: Display summary
        if pickle_success and json_success:
            display_vectorization_summary(
                vectorized_chunks, embeddings, model_name, pickle_path, json_path
            )
        
        # Return results
        result = {
            "success": True,
            "vectorized_chunks": vectorized_chunks,
            "embeddings": embeddings,
            "statistics": get_embedding_statistics(embeddings),
            "model_name": model_name,
            "files_saved": {
                "pickle": pickle_path if save_pickle and pickle_success else None,
                "json": json_path if save_json and json_success else None
            }
        }
        
        logger.info("‚úÖ Vectorization pipeline ho√†n t·∫•t th√†nh c√¥ng")
        return result
        
    except Exception as e:
        error_msg = f"Vectorization pipeline th·∫•t b·∫°i: {str(e)}"
        logger.error(error_msg)
        return {"success": False, "error": error_msg}


def load_vectorized_data(pickle_path: str) -> Optional[List[Dict[str, Any]]]:
    """
    Load vectorized data t·ª´ pickle file
    
    Args:
        pickle_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn pickle file
        
    Returns:
        List vectorized chunks ho·∫∑c None n·∫øu l·ªói
    """
    logger.info(f"Loading vectorized data from: {pickle_path}")
    
    try:
        with open(pickle_path, "rb") as f:
            vectorized_data = pickle.load(f)
        
        logger.info(f"Successfully loaded {len(vectorized_data)} vectorized chunks")
        return vectorized_data
        
    except Exception as e:
        logger.error(f"L·ªói khi load vectorized data: {str(e)}")
        return None
