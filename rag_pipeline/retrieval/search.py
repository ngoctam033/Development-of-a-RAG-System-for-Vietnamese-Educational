from typing import List, Dict, Any
import numpy as np
import faiss
from utils.logger import logger
import pandas as pd

def search_similar(
    query_text: str,
    store: Dict[str, Any],
    top_k: int = 3,
    header_path_filter: List[str] = None
) -> List[Dict[str, Any]]:
    """
    Find documents similar to the query text using FAISS, with optional header_path filtering.
    """
    model = store["embedding_model"]
    faiss_index = store["faiss_index"]
    vectorized_data = store["vectorized_data"]

    # logger.info("ğŸ” Báº¯t Ä‘áº§u tÃ¬m kiáº¿m tÃ i liá»‡u tÆ°Æ¡ng tá»±...")

    # Náº¿u cÃ³ header_path_filter, lá»c trÆ°á»›c vectorized_data
    if header_path_filter is not None:
        # logger.info(f"ğŸ” Äang lá»c vector theo header_path_filtr}")
        vectorized_data = filter_vectors_by_metadata(
            vectorized_data,
            {"header_path": header_path_filter}
        )
        # logger.info(f"âœ… Sá»‘ vector sau khi lá»c: {len(vectorized_data)}")
        # Náº¿u lá»c xong, cáº§n build láº¡i faiss_index cho táº­p nÃ y
        embeddings = np.array([item["embedding"] for item in vectorized_data]).astype('float32')
        if len(embeddings) > 0:
            faiss_index = faiss.IndexFlatIP(embeddings.shape[1])
            faiss.normalize_L2(embeddings)
            faiss_index.add(embeddings)
            # logger.info(f"âœ… ÄÃ£ build láº¡i FAISS index cho táº­p vector Ä‘Ã£ lá»c")
        else:
            # logger.warning("âš ï¸ KhÃ´ng cÃ²n vector nÃ o sau khi lá»c, tráº£ vá» rá»—ng.")
            return []

    # logger.info(f"ğŸ” Äang mÃ£ hÃ³a query vÃ  tÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»“ng FAISS cho: '{query_text}'")
    query_embedding = model.encode(query_text)
    query_embedding = np.array(query_embedding, dtype='float32').reshape(1, -1)
    faiss.normalize_L2(query_embedding)

    scores, indices = faiss_index.search(query_embedding, top_k)
    # logger.info(f"âœ… ÄÃ£ tÃ¬m kiáº¿m xong, tráº£ vá» top {top_k} káº¿t quáº£.")
    results = []
    for i, idx in enumerate(indices[0]):
        if idx >= len(vectorized_data):
            continue
        doc = vectorized_data[idx]
        results.append({
            "content": doc["content"],
            "metadata": doc["metadata"],
            "similarity_score": float(scores[0][i])
        })
        if len(results) >= top_k:
            break
    # logger.info(f"âœ… Sá»‘ káº¿t quáº£ tráº£ vá»: {len(results)}")
    # for item in results:
    #     logger.info(f"ğŸ” Káº¿t quáº£: {item['metadata']['header_path']} (Score: {item['similarity_score']})")
    return results

def filter_vectors_by_metadata(vectorized_data: List[Dict[str, Any]], metadata_filter: dict) -> List[Dict[str, Any]]:
    """
    Lá»c cÃ¡c vector theo Ä‘iá»u kiá»‡n metadata_filter nÃ¢ng cao.
    Náº¿u value lÃ  str, kiá»ƒm tra chuá»—i con; náº¿u khÃ´ng, so sÃ¡nh báº±ng tuyá»‡t Ä‘á»‘i.
    """
    # logger.info(f"ğŸ” Báº¯t Ä‘áº§u lá»c vector vá»›i metadata_filter: {metadata_filter}")
    # Sá»­ dá»¥ng pandas Ä‘á»ƒ lá»c cÃ¡c dict trong vectorized_data theo Ä‘iá»u kiá»‡n metadata_filter
    df = pd.DataFrame([item['metadata'] for item in vectorized_data])
    mask = pd.Series([True] * len(df))
    for k, v in metadata_filter.items():
        if k not in df.columns:
            mask = mask & False
            continue
        if isinstance(v, list):
            mask = mask & df[k].isin(v)
        elif isinstance(v, str):
            mask = mask & df[k].astype(str).str.contains(v)
        else:
            mask = mask & (df[k] == v)
    filtered_indices = df[mask].index.tolist()
    filtered = [vectorized_data[i] for i in filtered_indices]
    # logger.info(f"âœ… ÄÃ£ lá»c xong, cÃ²n láº¡i {len(filtered)} vector.")
    return filtered