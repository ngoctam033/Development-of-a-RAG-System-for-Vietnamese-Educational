# qa_interface.py
# ------------------------------------------------------------
# CLI H·ªèi-ƒê√°p d√πng Hybrid Retrieval (FAISS + BM25 + Reranker)
# ∆Øu ti√™n to√†n b·ªô v·∫•n ƒë·ªÅ v·ªÅ CH∆Ø∆†NG TR√åNH ƒê√ÄO T·∫†O
# - Tr√≠ch t·ª´/c·ª•m t·ª´ kh√≥a b·∫±ng KeywordExtractor (VN/EN) cho M·ªåI c√¢u h·ªèi gi√°o d·ª•c
# - Boost passage theo TR·ªåNG S·ªê keyword/phrase (domain > n-gram > token)
# - C·∫Øt context an to√†n (MAX_CONTEXT_CHARS)
# - Hi·ªÉn th·ªã l·ªói LLM r√µ r√†ng
# - Debug b·∫≠t/t·∫Øt qua SHOW_DEBUG
# ------------------------------------------------------------
import os
import sys
import numpy as np

# (tu·ª≥ ch·ªçn) ƒë·ªçc .env n·∫øu b·∫°n mu·ªën set API key qua file .env
try:
    from dotenv import load_dotenv  # pip install python-dotenv
    load_dotenv()
except Exception:
    pass

# B·∫£o ƒë·∫£m in Unicode ra terminal (Windows th∆∞·ªùng c·∫ßn)
try:
    sys.stdout.reconfigure(encoding="utf-8")
except Exception:
    pass

from config.prompt_templates import PROMPT_QA
from config.pipeline_config import FAISS_INDEX_PATH
from rag_pipeline.indexes.faiss_store import FaissStore
from rag_pipeline.retrieval.bm25_store import BM25Store
from rag_pipeline.retrieval.retriever import HybridRetriever
from rag_pipeline.generation.llm import GeminiGenerator
from rag_pipeline.retrieval.keyword_extractor import KeywordExtractor


# =========================================
# Helpers: Load corpus
# =========================================
def load_corpus_texts_and_metas(faiss_store: FaissStore):
    """
    ∆Øu ti√™n load t·ª´ .npy n·∫øu c√≥ (nhanh v√† ch·∫Øc ch·∫Øn).
    N·∫øu kh√¥ng c√≥, fallback l·∫•y t·ª´ faiss_store.meta (y√™u c·∫ßu l√∫c build ƒë√£ nh√©t text v√†o meta).
    """
    texts_path_candidates = [
        FAISS_INDEX_PATH + ".texts.npy",
        FAISS_INDEX_PATH + ".text.npy",
    ]
    metas_path_candidates = [
        FAISS_INDEX_PATH + ".metas.npy",   # khuy·∫øn ngh·ªã d√πng t√™n n√†y khi build
        FAISS_INDEX_PATH + ".meta.npy",    # fallback cho b·∫£n c≈©
    ]

    texts = None
    metas = None

    for p in texts_path_candidates:
        if os.path.exists(p):
            texts = np.load(p, allow_pickle=True).tolist()
            break

    for p in metas_path_candidates:
        if os.path.exists(p):
            metas = np.load(p, allow_pickle=True).tolist()
            break

    # Fallback: l·∫•y t·ª´ meta ƒë√£ l∆∞u c√πng index (n·∫øu build k√®m text)
    if texts is None:
        texts = []
        for m in getattr(faiss_store, "meta", []):
            if isinstance(m, dict):
                if "text" in m and isinstance(m["text"], str):
                    texts.append(m["text"])
                elif "chunk_text" in m and isinstance(m["chunk_text"], str):
                    texts.append(m["chunk_text"])
                else:
                    texts.append("")
            else:
                texts.append("")
    if metas is None:
        metas = getattr(faiss_store, "meta", [])

    return texts, metas


# =========================================
# Boosting theo KeywordExtractor
# =========================================
def _boost_score(text: str, header: str, doc: str, weight_map: dict) -> float:
    """
    C·ªông ƒëi·ªÉm theo TR·ªåNG S·ªê m·ªói keyword/c·ª•m n·∫øu xu·∫•t hi·ªán trong text/header/doc.
    - domain phrase (2.5) > n-gram (2.0) > token (1.0)
    - ∆Øu ti√™n text > header > doc (nh√¢n h·ªá s·ªë nh·ªè ƒë·ªÉ ph√¢n t·∫ßng nh·∫π)
    """
    s_text = (text or "").lower()
    s_head = (header or "").lower()
    s_doc  = (doc or "").lower()
    score = 0.0
    for k, w in weight_map.items():
        if k in s_text:
            score += w * 1.0
        if k in s_head:
            score += w * 0.5
        if k in s_doc:
            score += w * 0.25
    return score


def build_context(passages, corpus_texts, corpus_meta, kw_pairs):
    """
    Bi·∫øn c√°c ƒëo·∫°n top-k th√†nh context + danh s√°ch ngu·ªìn (ƒë·ªÉ in ra),
    c√≥ s·∫Øp x·∫øp ∆∞u ti√™n theo keyword/phrase + tr·ªçng s·ªë t·ª´ KeywordExtractor.
    """
    weight_map = {k.lower(): float(w) for k, w in (kw_pairs or [])}

    scored = []
    for p in passages:
        idx = p["idx"]
        text = corpus_texts[idx] if 0 <= idx < len(corpus_texts) else ""
        md = corpus_meta[idx] if 0 <= idx < len(corpus_meta) else {}
        header = ""
        doc = ""
        if isinstance(md, dict):
            header = md.get("header_path") or md.get("section_title") or md.get("section") or ""
            doc = (md.get("doc_name") or md.get("file_name") or md.get("document_name") or md.get("source") or "")
        boost = _boost_score(text, header, doc, weight_map)
        scored.append((boost, p, text, header, doc))

    # S·∫Øp x·∫øp: boost gi·∫£m d·∫ßn, r·ªìi theo rerank_score gi·∫£m d·∫ßn
    scored.sort(key=lambda x: (x[0], float(x[1].get("rerank_score", 0.0))), reverse=True)

    lines = []
    sources = []
    for _, p, text, header, doc in scored:
        lines.append(f"- ({doc or 'kh√¥ng r√µ t√†i li·ªáu'} | {header})\n{text}\n")
        sources.append({
            "doc": doc or "(kh√¥ng r√µ t√†i li·ªáu)",
            "path": header or "(kh√¥ng r√µ m·ª•c)",
            "score": round(float(p.get("rerank_score", 0.0)), 3),
        })

    context = "\n".join(lines)
    return context, sources


# =========================================
# Gemini call wrapper
# =========================================
class GeminiRunner:
    """
    Bao l·ªõp GeminiGenerator hi·ªán c√≥ v√† linh ho·∫°t g·ªçi c√°c method ph·ªï bi·∫øn:
    - generate_answer(prompt)
    - generate(prompt)
    - generate_content(prompt) (tr·∫£ v·ªÅ .text)
    """
    def __init__(self, api_key: str):
        self.inner = GeminiGenerator(api_key)
        # ƒë·ªÉ d√≤ng debug in ƒë√∫ng t√™n model
        self.model_name = getattr(self.inner, "model_name", os.getenv("GEMINI_MODEL_NAME", "?"))

    def ask(self, prompt: str) -> str:
        # Th·ª≠ c√°c ki·ªÉu method th∆∞·ªùng g·∫∑p theo th·ª© t·ª±
        if hasattr(self.inner, "generate_answer"):
            return self.inner.generate_answer(prompt)

        if hasattr(self.inner, "generate"):
            # C√≥ d·ª± √°n ƒë·∫∑t t√™n l√† generate(...)
            return self.inner.generate(prompt)

        if hasattr(self.inner, "model"):
            # Nhi·ªÅu repo tr·∫£ v·ªÅ .model c·ªßa Google Generative AI
            try:
                resp = self.inner.model.generate_content(prompt)
                if hasattr(resp, "text") and isinstance(resp.text, str):
                    return resp.text
                return str(resp)  # fallback
            except Exception as e:
                return f"[LLM error] {type(e).__name__}: {e}"

        return "[LLM error] Kh√¥ng t√¨m th·∫•y ph∆∞∆°ng th·ª©c g·ªçi Gemini ph√π h·ª£p. H√£y ƒë·∫£m b·∫£o GeminiGenerator c√≥ generate_answer(prompt) ho·∫∑c generate(prompt)."


# =========================================
# Main CLI
# =========================================
def run_cli():
    # 0) Ki·ªÉm tra API key
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è  GEMINI_API_KEY/GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")
        return

    # 1) Load FAISS index
    if not os.path.exists(FAISS_INDEX_PATH):
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {FAISS_INDEX_PATH}")
        print("üëâ H√£y ch·∫°y `python main.py` ƒë·ªÉ build index tr∆∞·ªõc.")
        return

    faiss_store = FaissStore()
    faiss_store.load()

    # 2) Load corpus texts & metas
    corpus_texts, corpus_meta = load_corpus_texts_and_metas(faiss_store)

    # 3) BM25 + HybridRetriever
    bm25_store = BM25Store(corpus_texts)
    retriever = HybridRetriever(faiss_store, bm25_store, corpus_texts)

    # 4) LLM (Gemini)
    llm = GeminiRunner(api_key)

    # 5) CLI banner
    print("\n" + "=" * 64)
    print("üß† H·ªÜ TH·ªêNG H·ªéI ƒê√ÅP CH∆Ø∆†NG TR√åNH ƒê√ÄO T·∫†O (Hybrid RAG)")
    print("Nh·∫≠p 'exit' ƒë·ªÉ tho√°t.")
    print("=" * 64)

    # Gi·ªõi h·∫°n context (c√≥ th·ªÉ override b·∫±ng env MAX_CONTEXT_CHARS)
    try:
        max_chars = int(os.getenv("MAX_CONTEXT_CHARS", "12000"))
    except Exception:
        max_chars = 12000

    SHOW_DEBUG = os.getenv("SHOW_DEBUG", "1") == "1"
    # Kh·ªüi t·∫°o KeywordExtractor 1 l·∫ßn cho to√†n phi√™n
    ke = KeywordExtractor()

    while True:
        try:
            question = input("\n‚ùì H·ªèi: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nT·∫°m bi·ªát!")
            break

        if not question:
            continue
        if question.lower() in ("exit", "quit"):
            print("T·∫°m bi·ªát!")
            break

        # 5.1) Retrieve (Hybrid + Reranker)
        try:
            top = retriever.search(question)
        except Exception as e:
            print(f"\n‚ö†Ô∏è L·ªói khi truy xu·∫•t: {e}")
            continue

        if not top:
            print("\nKh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu.")
            continue

        # 5.2) Tr√≠ch keyword/phrase c√≥ tr·ªçng s·ªë & Build context + ngu·ªìn
        try:
            kw_pairs = ke.extract(question)  # [(keyword_or_phrase, weight)]
            context, sources = build_context(top, corpus_texts, corpus_meta, kw_pairs=kw_pairs)
        except Exception as e:
            print(f"\n‚ö†Ô∏è L·ªói khi d·ª±ng context: {e}")
            continue

        # C·∫Øt b·ªõt context an to√†n cho LLM
        trimmed = False
        if len(context) > max_chars:
            context = context[:max_chars]
            trimmed = True

        # 5.3) Prompt v√† g·ªçi Gemini
        prompt = PROMPT_QA.format(context=context, question=question)
        try:
            answer = llm.ask(prompt)  # c√≥ th·ªÉ ch·ª©a "[LLM error]" ho·∫∑c "[LLM empty]"
        except Exception as e:
            print(f"\n‚ö†Ô∏è L·ªói khi g·ªçi Gemini: {e}")
            continue

        # 5.4) In k·∫øt qu·∫£
        print("\n" + "-" * 56)
        print("üìù C√¢u tr·∫£ l·ªùi:")
        print("-" * 56)
        if not isinstance(answer, str) or not answer.strip():
            print("[CLI] LLM tr·∫£ v·ªÅ r·ªóng.")
        else:
            print(answer)
            if answer.startswith("[LLM error]"):
                print("\nüëâ G·ª£i √Ω: ki·ªÉm tra t√™n model / version SDK ho·∫∑c ƒë·∫∑t GEMINI_MODEL_NAME=models/gemini-2.5-flash")
            elif answer.startswith("[LLM empty]"):
                print("\nüëâ G·ª£i √Ω: c√≥ th·ªÉ do safety/ƒë·ªô d√†i context. Gi·∫£m MAX_CONTEXT_CHARS ho·∫∑c xem safety_settings trong llm.py")

        print("\n" + "-" * 56)
        print("üîç Ngu·ªìn tham kh·∫£o:")
        print("-" * 56)
        for i, s in enumerate(sources, 1):
            doc = s.get("doc") or "(kh√¥ng r√µ t√†i li·ªáu)"
            path = s.get("path") or "(kh√¥ng r√µ m·ª•c)"
            score = s.get("score")
            print(f"[{i}] {doc} | {path}  (rerank: {score})")
        print("-" * 56)

        if SHOW_DEBUG:
            print(f"[debug] passages={len(top)} | context_chars={len(context)}"
                  f" | model={getattr(llm, 'model_name', os.getenv('GEMINI_MODEL_NAME','?'))} | trimmed={trimmed}")


if __name__ == "__main__":
    run_cli()
